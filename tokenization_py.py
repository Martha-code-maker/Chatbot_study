# -*- coding: utf-8 -*-
"""Tokenization.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a1kVAD5riJoNC1oFzlPl7j1d3KbAmquC

1. 단어 토큰화(Word Tokenization)
"""

from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
import nltk
nltk.download('punkt')

"""Don't = Do + n't

Jone's = Jone + 's
"""

print('단어 토큰화1 : ', word_tokenize("Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop."))

"""Don't = Don + ' + t

Jone's = Jone + ' + s
"""

print('단어 토큰화2 : ', WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop."))

"""모든 알파벳 소문자화

구두점('.', ',', '!'등) 제거

don't & jone's : '''보존
"""

print('단어 토큰화3 : ', text_to_word_sequence("Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop."))

"""2. 표준 토큰화 예제"""

from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print("트리뱅크 워드토크나이저 : ", tokenizer.tokenize(text))

"""3. 문장 토큰화"""

from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))

pip install kss

import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))

"""4. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"""

import nltk
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print("단어 토큰화 :" , tokenized_sentence)
print('품사 태깅 : ', pos_tag(tokenized_sentence))

pip install konlpy

from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()       #형태소 분석기
kkma = Kkma()

print('OKT 형태소 분석 :',okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 품사 태깅 :',okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 명사 추출 :',okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요")) 
print()
print('꼬꼬마 형태소 분석 :',kkma.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 품사 태깅 :',kkma.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 명사 추출 :',kkma.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))