{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Padding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPB0W7SUSr/NxMtPORkJla9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martha-code-maker/Chatbot_study/blob/main/Padding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. zero padding"
      ],
      "metadata": {
        "id": "x14kesCOGirA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73kuVsfu_ibi",
        "outputId": "a886bd2a-fce1-4a3e-df86-580ba0e85826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "sentences = sent_tokenize(raw_text)"
      ],
      "metadata": {
        "id": "NUrhh3kdAt6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence: \n",
        "        word = word.lower() \n",
        "        if word not in stop_words:  \n",
        "            if len(word) > 2:  \n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result) \n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm6Xf-IHAus5",
        "outputId": "b76e108c-ed0f-4a6f-ed7d-4f056a3f98b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raoqqNIDCEoX",
        "outputId": "d83cf002-fe48-476f-a1e8-1475632d4147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "\n",
        "for sentences in encoded:\n",
        "  while len(sentences) < max_len:\n",
        "    sentences.append(0)\n",
        "\n",
        "padded = np.array(encoded)\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs-tfezxECMN",
        "outputId": "924458db-cdf4-4292-c1a8-8605ceabb784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  0  0  0  0  0]\n",
            " [ 1  8  5  0  0  0  0]\n",
            " [ 1  3  5  0  0  0  0]\n",
            " [ 9  2  0  0  0  0  0]\n",
            " [ 2  4  3  2  0  0  0]\n",
            " [ 3  2  0  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  2  0  0  0  0]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 케라스 zero 패딩"
      ],
      "metadata": {
        "id": "LRw5lMHHGrwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPlBosneGusR",
        "outputId": "08fa86d7-f040-418a-d166-d76d64015fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded , padding = 'post', maxlen = 5)\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob4zgDztLCSF",
        "outputId": "3ad5a334-1beb-488b-9384-059c6feea63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  0  0  0]\n",
            " [ 1  8  5  0  0]\n",
            " [ 1  3  5  0  0]\n",
            " [ 9  2  0  0  0]\n",
            " [ 2  4  3  2  0]\n",
            " [ 3  2  0  0  0]\n",
            " [ 1  4  6  0  0]\n",
            " [ 1  4  6  0  0]\n",
            " [ 1  4  2  0  0]\n",
            " [ 3  2 10  1 11]\n",
            " [ 1 12  3 13  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 숫자로 패딩"
      ],
      "metadata": {
        "id": "54HYrQQzOU1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_value = len(tokenizer.word_index) + 1    #현재 사용된 정수들과 겹치지 않게 하기 위해 단어 집합 크기 + 1한 크기를 사용\n",
        "print(last_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TQEL0hNOW9C",
        "outputId": "35f6ddf6-324e-4f3e-8e3d-c06239dde6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', value = last_value)\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8x01VBJOpet",
        "outputId": "baec450f-31ec-49ec-e321-5b2d01cdb074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5 14 14 14 14 14]\n",
            " [ 1  8  5 14 14 14 14]\n",
            " [ 1  3  5 14 14 14 14]\n",
            " [ 9  2 14 14 14 14 14]\n",
            " [ 2  4  3  2 14 14 14]\n",
            " [ 3  2 14 14 14 14 14]\n",
            " [ 1  4  6 14 14 14 14]\n",
            " [ 1  4  6 14 14 14 14]\n",
            " [ 1  4  2 14 14 14 14]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13 14 14 14]]\n"
          ]
        }
      ]
    }
  ]
}
